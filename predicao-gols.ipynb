{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploração dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de instâncias: 380\n",
      "Número de atributos: 26\n",
      "match_id                    0\n",
      "stage                       0\n",
      "date                        0\n",
      "team_name_home              0\n",
      "team_name_away              0\n",
      "team_home_score             0\n",
      "team_away_score             0\n",
      "possession_home             0\n",
      "possession_away             0\n",
      "total_shots_home            0\n",
      "total_shots_away            0\n",
      "shots_on_target_home        0\n",
      "shots_on_target_away        0\n",
      "duels_won_home              0\n",
      "duels_won_away              0\n",
      "prediction_team_home_win    0\n",
      "prediction_draw             0\n",
      "prediction_team_away_win    0\n",
      "prediction_quantity         0\n",
      "location                    0\n",
      "lineup_home                 0\n",
      "lineup_away                 0\n",
      "player_names_home           0\n",
      "player_numbers_home         0\n",
      "player_names_away           0\n",
      "player_numbers_away         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Carregando o conjunto de dados\n",
    "data = pd.read_csv('./matches_brasileirao_serie_a_2022.csv')\n",
    "\n",
    "# verficando a quantidade de instâncias\n",
    "instances = data.shape[0]\n",
    "print('Número de instâncias: ' + str(instances))\n",
    "\n",
    "# verificando a quantidade de atributos\n",
    "attributes = data.shape[1]\n",
    "print('Número de atributos: ' + str(attributes))\n",
    "\n",
    "# Verificando a presença de dados ausentes\n",
    "print(data.isna().sum())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  team_name_home  team_home_score  gols_marcados_temporada   \n",
      "0     América MG                1                        1  \\\n",
      "1  RB Bragantino                0                        0   \n",
      "2  Internacional                3                        3   \n",
      "3          Goiás                0                        0   \n",
      "4         Cuiabá                2                        2   \n",
      "\n",
      "   gols_sofridos_temporada  \n",
      "0                        1  \n",
      "1                        1  \n",
      "2                        0  \n",
      "3                        4  \n",
      "4                        1  \n"
     ]
    }
   ],
   "source": [
    "# Separando atributos desejáveis\n",
    "parsed_data = data.drop(\n",
    "  labels=['stage', 'date', 'location', 'lineup_home', 'lineup_away', 'player_names_home', 'player_numbers_home', 'player_names_away', 'player_numbers_away', 'prediction_team_home_win', 'prediction_draw', 'prediction_team_away_win', 'prediction_quantity'],\n",
    "  axis=1)\n",
    "\n",
    "parsed_data['both_scores_nonzero'] = np.where((data['team_home_score'] != 0) & (data['team_away_score']!=0), 1, 0)\n",
    "\n",
    "# Criando a variável 'gols_marcados_temporada' para contabilziar a quantidade de gols marcados na tmeporada\n",
    "parsed_data['gols_marcados_temporada'] = parsed_data.groupby('team_name_home')['team_home_score'].cumsum()\n",
    "\n",
    "# Criando a variável 'gols_sofridos_temporada' para contabilziar a quantidade de gols sofridos na tmeporada\n",
    "parsed_data['gols_sofridos_temporada'] = parsed_data.groupby('team_name_home')['team_away_score'].cumsum()\n",
    "\n",
    "print(parsed_data[['team_name_home', 'team_home_score', 'gols_marcados_temporada', 'gols_sofridos_temporada']].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividindo os dados em um conjunto de treinamento e testes (80% por 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do conjunto de treinamento: 304\n",
      "Tamanho do conjunto de teste: 76\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divisão dos dados em conjunto de treinamento e teste\n",
    "train_data, test_data = train_test_split(parsed_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verificando o tamanho dos conjuntos de treinamento e teste\n",
    "print(\"Tamanho do conjunto de treinamento:\", len(train_data))\n",
    "print(\"Tamanho do conjunto de teste:\", len(test_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo o algotimo de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo: 0.5657894736842105\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Definindo as variáveis de entrada (features) e o alvo (target)\n",
    "features = ['gols_marcados_temporada', 'gols_sofridos_temporada']\n",
    "target = 'both_scores_nonzero'\n",
    "\n",
    "# Separando as features e o alvo do conjunto de treinamento e teste\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[target]\n",
    "\n",
    "# Criando e treinando o modelo Decision Tree\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Fazendo previsões no conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Acurácia do modelo:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média da validação cruzada: 0.47693989071038245\n",
      "Desvio padrão da validação cruzada: 0.05448983489021858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Aplicando validação cruzada\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "\n",
    "# Exibindo a acurácia média e desvio padrão\n",
    "print(\"Acurácia média da validação cruzada:\", scores.mean())\n",
    "print(\"Desvio padrão da validação cruzada:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previsões:\n",
      "Amostra 1 : Ambos marcam\n",
      "Amostra 2 Ambos não marcam\n",
      "Amostra 3 : Ambos marcam\n",
      "Amostra 4 Ambos não marcam\n",
      "Amostra 5 : Ambos marcam\n",
      "Amostra 6 Ambos não marcam\n",
      "Amostra 7 : Ambos marcam\n",
      "Amostra 8 : Ambos marcam\n",
      "Amostra 9 Ambos não marcam\n",
      "Amostra 10 Ambos não marcam\n",
      "Amostra 11 : Ambos marcam\n",
      "Amostra 12 : Ambos marcam\n",
      "Amostra 13 : Ambos marcam\n",
      "Amostra 14 Ambos não marcam\n",
      "Amostra 15 : Ambos marcam\n",
      "Amostra 16 : Ambos marcam\n",
      "Amostra 17 : Ambos marcam\n",
      "Amostra 18 Ambos não marcam\n",
      "Amostra 19 Ambos não marcam\n",
      "Amostra 20 : Ambos marcam\n",
      "Amostra 21 Ambos não marcam\n",
      "Amostra 22 Ambos não marcam\n",
      "Amostra 23 : Ambos marcam\n",
      "Amostra 24 : Ambos marcam\n",
      "Amostra 25 : Ambos marcam\n",
      "Amostra 26 : Ambos marcam\n",
      "Amostra 27 : Ambos marcam\n",
      "Amostra 28 : Ambos marcam\n",
      "Amostra 29 : Ambos marcam\n",
      "Amostra 30 Ambos não marcam\n",
      "Amostra 31 Ambos não marcam\n",
      "Amostra 32 : Ambos marcam\n",
      "Amostra 33 Ambos não marcam\n",
      "Amostra 34 Ambos não marcam\n",
      "Amostra 35 Ambos não marcam\n",
      "Amostra 36 Ambos não marcam\n",
      "Amostra 37 Ambos não marcam\n",
      "Amostra 38 : Ambos marcam\n",
      "Amostra 39 Ambos não marcam\n",
      "Amostra 40 : Ambos marcam\n",
      "Amostra 41 Ambos não marcam\n",
      "Amostra 42 : Ambos marcam\n",
      "Amostra 43 : Ambos marcam\n",
      "Amostra 44 : Ambos marcam\n",
      "Amostra 45 Ambos não marcam\n",
      "Amostra 46 : Ambos marcam\n",
      "Amostra 47 : Ambos marcam\n",
      "Amostra 48 Ambos não marcam\n",
      "Amostra 49 : Ambos marcam\n",
      "Amostra 50 Ambos não marcam\n",
      "Amostra 51 Ambos não marcam\n",
      "Amostra 52 Ambos não marcam\n",
      "Amostra 53 : Ambos marcam\n",
      "Amostra 54 : Ambos marcam\n",
      "Amostra 55 : Ambos marcam\n",
      "Amostra 56 Ambos não marcam\n",
      "Amostra 57 Ambos não marcam\n",
      "Amostra 58 Ambos não marcam\n",
      "Amostra 59 : Ambos marcam\n",
      "Amostra 60 Ambos não marcam\n",
      "Amostra 61 : Ambos marcam\n",
      "Amostra 62 Ambos não marcam\n",
      "Amostra 63 : Ambos marcam\n",
      "Amostra 64 : Ambos marcam\n",
      "Amostra 65 Ambos não marcam\n",
      "Amostra 66 Ambos não marcam\n",
      "Amostra 67 Ambos não marcam\n",
      "Amostra 68 Ambos não marcam\n",
      "Amostra 69 : Ambos marcam\n",
      "Amostra 70 Ambos não marcam\n",
      "Amostra 71 Ambos não marcam\n",
      "Amostra 72 Ambos não marcam\n",
      "Amostra 73 : Ambos marcam\n",
      "Amostra 74 : Ambos marcam\n",
      "Amostra 75 Ambos não marcam\n",
      "Amostra 76 Ambos não marcam\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dados de teste ou novos dados para prever\n",
    "new_data = test_data \n",
    "\n",
    "# Aplicando as mesmas etapas de pré-processamento nas novas entradas\n",
    "# ...\n",
    "# Pré-processamento dos novos dados\n",
    "\n",
    "# Realizando as previsões\n",
    "new_features = new_data[features]\n",
    "predictions = model.predict(new_features)\n",
    "\n",
    "# Exibindo as previsões\n",
    "print(\"Previsões:\")\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(\"Amostra\", i+1, \": Ambos marcam\" if pred == 1 else \"Ambos não marcam\")\n",
    "\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
